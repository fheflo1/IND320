# IND320: Data to Decisions

## Project Description

This repository accompanies the IND320 course at NMBU.  
Throughout the course, I work with data collection, processing, visualization, and decision support.

### The project combines:

- an **ETL pipeline** (Elhub + meteorological data) using **PySpark** and **Cassandra/MongoDB**
- an interactive **Streamlit dashboard** for analysis and decision support

### Main tasks:

- Fetch, clean and store energy data from **Elhub** and weather data from **Open-Meteo**
- Build **bronzeâ€“silverâ€“gold** style data pipelines
- Store data in **Cassandra** (raw/processed) and expose analysis tables via **MongoDB**
- Visualize key insights in an interactive **Streamlit** dashboard
- Run **forecasting** (SARIMAX) and weatherâ€“energy analyses

---

## Streamlit Dashboard

### App Link

ðŸ‘‰ **Deployed app:** <https://ind320-fheflo1.streamlit.app/>

The dashboard presents energy production/consumption and meteorological data in a Power-BIâ€“like interface to support data-driven decisions.

### Navigation (pages)

The app is organized into the following pages:

- **Home** â€“ short description, latest snapshot and navigation help  
- **Data Overview** â€“ high-level overview of available production/consumption data  
- **Energy Overview** â€“ production & consumption aggregations and time-series views  
- **Energy STL & Spectrogram** â€“ STL decomposition and frequency analysis of production  
- **Energy Forecast** â€“ SARIMAX forecasting using weather as exogenous variables  
- **Weather Overview** â€“ descriptive statistics and plots of meteo data  
- **Weather Outliers & Anomalies** â€“ outlier and anomaly detection on weather series  
- **Weather & Energy Correlation** â€“ sliding-window correlations between weather and energy  
- **Map of Price Areas** â€“ interactive Leaflet map with price areas and linked energy data  
- **Snow Drift** â€“ snow drift calculation and visualization based on meteo + map selection  

---

## Running the Dashboard Locally

From the project root:

```bash
pip install -r requirements.txt

# Run the Streamlit app
streamlit run apps/Home.py
The app expects that production, consumption and meteo silver tables have been loaded into MongoDB (via the notebooks described below) and are accessible through the helper functions in src/db/mongo_elhub.py.

Data Pipelines
Both Elhub and meteorological data follow a layered pipeline, implemented in notebooks and reusable Python modules.

Elhub: Bronze â†’ Silver â†’ Gold
Location: notebooks/elhub/

01_cassandra_setup.ipynb
Create keyspaces and tables in Cassandra for raw and processed Elhub data.

02_elhub_bronze.ipynb
Fetch raw production/consumption data from the Elhub API and store it in Cassandra (bronze).

03_elhub_silver.ipynb
Clean, standardize and enrich bronze data into analysis-ready silver tables.

04_elhub_gold.ipynb
Aggregate silver data into higher-level, business-ready gold tables (e.g. daily/weekly summaries).

05_db_connections.ipynb
Connect Cassandra â†” MongoDB and push relevant silver/gold tables into MongoDB for use by Streamlit.

Meteo: Bronze â†’ Silver
Location: notebooks/meteo/

01_meteo_bronze.ipynb
Download raw meteorological data (e.g. from Open-Meteo) and store in bronze tables.

02_meteo_silver.ipynb
Clean, interpolate and reshape meteo data into silver tables aligned with the Elhub data.

Reports
Location: notebooks/reports/

01_report.ipynb, 02_report.ipynb, 03_report.ipynb, 04_report.ipynb
Jupyter notebooks used to generate course deliverables; each has a corresponding exported *.pdf.

Project Structure
Using the current layout (see screenshots in the repo), the important pieces are:
```
```bash
Kopier kode
.
â”œâ”€ apps/
â”‚  â”œâ”€ .streamlit/
â”‚  â”‚   â”œâ”€ config.toml              # Streamlit theme/config
â”‚  â”‚   â””â”€ secrets.toml             # Local secrets (ignored in Git)
â”‚  â”œâ”€ pages/
â”‚  â”‚   â”œâ”€ 01_Data_Overview.py
â”‚  â”‚   â”œâ”€ 02_Energy_Overview.py
â”‚  â”‚   â”œâ”€ 03_Energy_STL_&_Spectrogram.py
â”‚  â”‚   â”œâ”€ 04_Energy_Forecast.py
â”‚  â”‚   â”œâ”€ 05_Weather_Overview.py
â”‚  â”‚   â”œâ”€ 06_Weather_Outliers_&_Anomalies.py
â”‚  â”‚   â”œâ”€ 07_Weather_&_Energy_Correlation.py
â”‚  â”‚   â”œâ”€ 08_Map_of_Price_Areas.py
â”‚  â”‚   â””â”€ 09_Snow_Drift.py
â”‚  â””â”€ Home.py                      # Entry point for the dashboard
â”‚
â”œâ”€ notebooks/
â”‚  â”œâ”€ elhub/
â”‚  â”‚   â”œâ”€ 01_cassandra_setup.ipynb
â”‚  â”‚   â”œâ”€ 02_elhub_bronze.ipynb
â”‚  â”‚   â”œâ”€ 03_elhub_silver.ipynb
â”‚  â”‚   â”œâ”€ 04_elhub_gold.ipynb
â”‚  â”‚   â””â”€ 05_db_connections.ipynb
â”‚  â”œâ”€ meteo/
â”‚  â”‚   â”œâ”€ 01_meteo_bronze.ipynb
â”‚  â”‚   â””â”€ 02_meteo_silver.ipynb
â”‚  â””â”€ reports/
â”‚      â”œâ”€ 01_report.ipynb / 01_report.pdf
â”‚      â”œâ”€ 02_report.ipynb / 02_report.pdf
â”‚      â”œâ”€ 03_report.ipynb / 03_report.pdf
â”‚      â””â”€ 04_report.ipynb
â”‚
â”œâ”€ src/
â”‚  â”œâ”€ analysis/
â”‚  â”‚   â”œâ”€ __init__.py
â”‚  â”‚   â”œâ”€ anomaly_detection.py     # Weather/energy outlier logic
â”‚  â”‚   â””â”€ plots.py                 # General plotting utilities (energy side)
â”‚  â”‚
â”‚  â”œâ”€ api/
â”‚  â”‚   â”œâ”€ __init__.py
â”‚  â”‚   â”œâ”€ elhub_api.py             # Functions for calling the Elhub API
â”‚  â”‚   â””â”€ meteo_api.py             # Functions for calling the meteo API
â”‚  â”‚
â”‚  â”œâ”€ data/
â”‚  â”‚   â”œâ”€ __init__.py
â”‚  â”‚   â””â”€ load_data.py             # Helpers to load data into the app
â”‚  â”‚
â”‚  â”œâ”€ db/
â”‚  â”‚   â””â”€ mongo_elhub.py           # MongoDB access for production/consumption tables
â”‚  â”‚
â”‚  â”œâ”€ forecast/
â”‚  â”‚   â”œâ”€ __init__.py
â”‚  â”‚   â””â”€ sarimax_utils.py         # SARIMAX prep/fit/forecast utilities
â”‚  â”‚
â”‚  â””â”€ ui/
â”‚      â”œâ”€ __init__.py
â”‚      â”œâ”€ sidebar_controls.py      # Shared sidebar controls for several pages
â”‚      â”œâ”€ app_state.py             # Small helpers for managing Streamlit session state
â”‚      â””â”€ plots_meteo_report.py    # Meteo-specific plotting for reports/dashboard
â”‚
â”œâ”€ .cache.sqlite                   # Streamlit cache (ignored / local)
â”œâ”€ .gitignore
â”œâ”€ README.md                       # This file
â””â”€ requirements.txt
```
## Technology Stack

- **Apache Cassandra** â€“ main storage for raw and processed Elhub data used in the ETL steps  
- **MongoDB** â€“ serves cleaned/silver/gold tables to the Streamlit dashboard  
- **Apache Spark (PySpark)** â€“ data transformation and ETL in the notebooks  
- **Streamlit** â€“ interactive dashboard and visualizations  
- **Python** â€“ API calls, cleaning, modelling and analysis  
- **Statsmodels** â€“ SARIMAX forecasting  
- **Pandas / NumPy** â€“ data wrangling and numeric analysis

---

## Getting Started (ETL + Dashboard)

### Set up databases
Start local instances of **Cassandra** and **MongoDB** (e.g. via Docker or local services) before running the pipelines.

### Run Elhub pipeline
- notebooks/elhub/01_cassandra_setup.ipynb â€” create keyspaces and tables in **Cassandra**  
- notebooks/elhub/02_elhub_bronze.ipynb â€” ingest raw Elhub data into bronze tables  
- notebooks/elhub/03_elhub_silver.ipynb â€” clean, standardize and enrich into silver tables  
- notebooks/elhub/04_elhub_gold.ipynb â€” aggregate silver into business-ready gold tables  
- notebooks/elhub/05_db_connections.ipynb â€” push relevant silver/gold tables into **MongoDB**

### Run Meteo pipeline
- notebooks/meteo/01_meteo_bronze.ipynb â€” ingest raw meteorological data (e.g. Openâ€‘Meteo)  
- notebooks/meteo/02_meteo_silver.ipynb â€” clean, interpolate and align meteo data to silver

### Install dependencies and run the dashboard
```bash
pip install -r requirements.txt
streamlit run apps/Home.py
```
- Ensure the silver tables (production, consumption and meteo) are available in **MongoDB** for the dashboard to load.
- Use the app sidebar to navigate pages, interact with filters, STL decomposition, forecasts, correlations and maps.

```bash
pip install -r requirements.txt
streamlit run apps/Home.py
```
### Explore the dashboard
- Use the navigation in the left sidebar to open the different analysis pages.
- Interact with filters, STL decomposition, forecast controls, correlation sliders, and maps to explore the data.